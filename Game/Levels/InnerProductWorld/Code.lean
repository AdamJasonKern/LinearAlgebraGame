import Game.Levels.LinearIndependenceSpanWorld.Level09

-- Minimal Mathlib imports for Lean 4.7.0 - avoiding conflicts with custom definitions
import Mathlib.Data.Complex.Basic
import Mathlib.Data.Real.Basic

open Function Set VectorSpace


instance  [Field K] [AddCommGroup V] [VectorSpace K V] : DistribSMul K V where
  smul_add := smul_add
  smul_zero := smul_zero_v K V

class InnerProductSpace_real_v (V : Type) [AddCommGroup V] [VectorSpace ℝ V] where
  inner : V → V → ℝ

  -- Properties are simpler for real case
  inner_self_nonneg : ∀ (v : V), 0 ≤ inner v v
  inner_self_eq_zero : ∀ (v : V), inner v v = 0 ↔ v = 0
  inner_add_left : ∀ (u v w : V), inner (u + v) w = inner u w + inner v w
  inner_smul_left : ∀ (a : ℝ) (v w : V), inner (a • v) w = a * inner v w
  inner_symm : ∀ (v w : V), inner v w = inner w v-- Import complex numbers from mathlib




-- Section 6: InnerProduct

open ComplexConjugate

-- Inner product space definition for complex vector spaces
class InnerProductSpace_v (V : Type) [AddCommGroup V] [VectorSpace ℂ V] where
  inner : V → V → ℂ

  -- 1. Positivity: ⟨v,v⟩ is real and non-negative
  inner_self_im_zero : ∀ (v : V), (inner v v).im = 0
  inner_self_nonneg : ∀ (v : V), 0 ≤ (inner v v).re

  -- 2. Definiteness: ⟨v,v⟩ = 0 iff v = 0
  inner_self_eq_zero : ∀ (v : V), inner v v = 0 ↔ v = 0

  -- 3. Additivity in first slot: ⟨u + v, w⟩ = ⟨u, w⟩ + ⟨v, w⟩
  inner_add_left : ∀ (u v w : V), inner (u + v) w = inner u w + inner v w

  -- 4. Homogeneity in first slot: ⟨a • v, w⟩ = a * ⟨v, w⟩
  inner_smul_left : ∀ (a : ℂ) (v w : V), inner (a • v) w = a * inner v w

  --#check
  -- 5. Conjugate symmetry: ⟨v, w⟩ = conj(⟨w, v⟩)
  inner_conj_symm : ∀ (v w : V), inner v w = conj (inner w v)



-- Custom notation for our inner product
notation "⟪" x "," y "⟫" => InnerProductSpace_v.inner x y


variable {V : Type} [AddCommGroup V] [VectorSpace ℂ V] [DecidableEq V] [InnerProductSpace_v V]

-- Create theorem aliases for class fields that are used in TheoremDoc
theorem inner_self_im_zero (v : V) : (⟪v, v⟫).im = 0 := 
  InnerProductSpace_v.inner_self_im_zero v

theorem inner_self_nonneg (v : V) : 0 ≤ (⟪v, v⟫).re := 
  InnerProductSpace_v.inner_self_nonneg v

theorem inner_self_eq_zero (v : V) : ⟪v, v⟫ = 0 ↔ v = 0 := 
  InnerProductSpace_v.inner_self_eq_zero v

theorem inner_add_left (u v w : V) : ⟪u + v, w⟫ = ⟪u, w⟫ + ⟪v, w⟫ := 
  InnerProductSpace_v.inner_add_left u v w

theorem inner_smul_left (a : ℂ) (v w : V) : ⟪a • v, w⟫ = a * ⟪v, w⟫ := 
  InnerProductSpace_v.inner_smul_left a v w

theorem inner_conj_symm (v w : V) : ⟪v, w⟫ = conj (⟪w, v⟫) := 
  InnerProductSpace_v.inner_conj_symm v w

--Level 5
--Title "Inner Product of a Vector with Itself is Real"
--Introduction " In a complex inner product space, $\langle v,v\rangle$ is always a real number. Here “real” means that $\langle v,v\rangle$ equals its own complex conjugate, or equivalently its imaginary part is zero. This theorem formally states that $\langle v,v\rangle = (\langle v,v\rangle.\mathrm{re} : \mathbb{C})$, i.e. it equals a purely real number. This is a direct consequence of the positivity axiom and is important because it justifies defining $|v| = \sqrt{\langle v,v\rangle.\mathrm{re}}$."
--Hint "To prove a complex number equals a real number, show that its imaginary part is zero. Here, start by applying the Complex number extensionality (comparing real and imaginary parts). The imaginary part of $\langle v,v\rangle$ is $0$ by the inner product positivity property, and the real part trivially equals itself. Thus $\langle v,v\rangle$ equals a real number with the same real part."

theorem inner_self_real : ∀ (v : V), ⟪v,v⟫ = (⟪v,v⟫.re : ℂ):= by
  intro v
  apply Complex.ext
  rfl
  rw [inner_self_im_zero v]
  simp

--Level 6
--Title "Inner Product Distributes over Subtraction (First Argument)"
--Introduction "This theorem extends linearity of the inner product in the first argument to subtraction: $\langle u - v, w\rangle = \langle u,w\rangle - \langle v,w\rangle$. The idea is that since the inner product is additive in the first argument and homogeneous, it should handle differences properly (where $-v$ is treated as $(-1)\cdot v$). This is theoretically significant as it allows one to break apart inner products of differences, which is useful in proofs and derivations (e.g., the proof of the Pythagorean theorem in inner product spaces)."
--Hint "Rewrite the difference as an addition with a scalar multiple: $u - v = u + (-1)\cdot v$. Then use additivity: $\langle u + (-1)\cdot v, w \rangle = \langle u,w\rangle + \langle -1 \cdot v, w\rangle$. Next, apply homogeneity: $\langle -1 \cdot v, w\rangle = -1 * \langle v,w\rangle = -\langle v,w\rangle$. Combining these results yields $\langle u - v, w\rangle = \langle u,w\rangle - \langle v,w\rangle$."

theorem inner_minus_left : ∀ (u v w : V), ⟪u - v, w⟫ = ⟪u, w⟫ - ⟪v, w⟫ := by
  intro u v w
  have : u-v=u+(-1 :ℂ)• v := by
    refine Eq.symm (add_eq_of_eq_sub' ?_)
    rw [@sub_sub_cancel_left]
    exact neg_one_smul_v ℂ V v
  rw [this]
  rw [InnerProductSpace_v.inner_add_left]
  rw [InnerProductSpace_v.inner_smul_left]
  ring

--Level 7
--Title "Complex Conjugation is Injective"
--Introduction "This lemma asserts that the complex conjugation operation is injective: if $\overline{a} = \overline{b}$, then $a = b$ for any complex numbers $a,b$. In other words, different complex numbers cannot have the same conjugate. The significance in the context of inner product spaces is that one can take the complex conjugate of both sides of an inner-product equation and not lose information—crucial for results that rely on conjugate symmetry (like showing additivity in the second argument of the inner product)."
--Hint "If $\overline{a} = \overline{b}$, take the conjugate of both sides of this equation. Since taking conjugate twice gives back the original number ($\overline{\overline{z}} = z$), applying conjugation to both sides yields $a = b$."

-- Helper theorem: Complex conjugation is injective
theorem conj_inj {a b : ℂ} : conj a = conj b → a = b := by
  intro h
  -- Apply conj to both sides, using conj ∘ conj = id
  have : conj (conj a) = conj (conj b) := by
    rw [h]
  rw [Complex.conj_conj, Complex.conj_conj] at this
  exact this


--Level 8
--Title "Conjugation Distributes over Addition and Multiplication"
--Introduction "For any complex numbers $a$ and $b$, $\overline{a + b} = \overline{a} + \overline{b}$. This property means the conjugate of a sum is the sum of the conjugates. It’s a basic property of complex arithmetic, reflecting that conjugation is a field automorphism that respects addition. In inner product proofs, this is used to break the conjugate of a sum of inner products into the sum of conjugates of each inner product.
--Similarly,  For any complex numbers $a$ and $b$, $\overline{a \cdot b} = \overline{a} \cdot \overline{b}$. The conjugate of a product equals the product of the conjugates. This is another fundamental property of complex conjugation, indicating it preserves the multiplicative structure as well. In inner product contexts, this property helps simplify conjugates of products, for instance when pulling scalars out of an inner product."
--Hint "Use the definitions: write $a = x + yi$ and $b = u + vi$ (with $x,y,u,v \in \mathbb{R}$). Then $a+b = (x+u) + (y+v)i$. The conjugates are $\overline{a} = x - yi$, $\overline{b} = u - vi$. Add them: $\overline{a} + \overline{b} = (x+u) - (y+v)i$, which equals $\overline{a+b}$."

-- Conjugation distributes over addition
theorem conj_add (a b : ℂ) : conj (a + b) = conj a + conj b := by
  apply Complex.ext
  dsimp
  dsimp
  rw [@neg_add]

--Hint "Again, expand $a$ and $b$ in terms of real and imaginary parts or use known component formulas. You can also reason that $\overline{a b}$ and $\overline{a},\overline{b}$ have the same real part ($\Re(\overline{a b}) = \Re(a)\Re(b) - \Im(a)\Im(b)$ equals $\Re(\overline{a})\Re(\overline{b}) - \Im(\overline{a})\Im(\overline{b})$) and similarly the same imaginary part (with a sign flip), hence they are equal."
-- Conjugation distributes over multiplication
theorem conj_mull (a b : ℂ) : conj (a * b) = conj a * conj b := by
  apply Complex.ext
  · simp [Complex.mul_re, Complex.conj_re, Complex.conj_im]
  · simp [Complex.mul_im, Complex.conj_re, Complex.conj_im]


--Level 9
--Title "Conjugation of Zero is Zero"
--Introduction "$\overline{0} = 0$. This simple statement says the complex conjugate of $0 + 0i$ is itself, $0 - 0i$, which is just $0$. It’s a sanity check property of conjugation, ensuring that it respects the additive identity. This is useful to conclude results like $\overline{\langle v,0\rangle} = 0$ implying $\langle 0,v\rangle = 0$."
--Hint "Directly from the definition of conjugation: $0$ has real part $0$ and imaginary part $0$, so its conjugate $\overline{0 + 0i}$ is $0 - 0i$, which equals $0$."

theorem conj_zero : conj (0 : ℂ) = 0 := by
  apply Complex.ext <;> simp [Complex.conj_re, Complex.conj_im]

--Level 10
--Title "Inner Product of $v$ with $v$ Equals Its Real Part"
--Introduction "This is another way to state that $\langle v,v\rangle$ is real: $\langle v, v\rangle = \langle v,v\rangle.\mathrm{re}$ (viewed as a complex number on the right). Here it’s phrased as an equality in $\mathbb{C}$ between the inner product and a real number. This reiteration is often useful when one needs to replace an inner product with a real value in a calculation (like in the orthogonal decomposition proof)."
--Hint "Use the fact that $\langle v,v\rangle$ has zero imaginary part. By showing $(\langle v,v\rangle - \langle v,v\rangle.\mathrm{re})$ has zero real and imaginary parts, you conclude they are equal. Essentially, combine $\langle v,v\rangle.\mathrm{im} = 0$ with the identity of the real part."

-- Useful lemma: ⟨v,v⟩ is real (combines with definiteness)
theorem inner_self_re_v (V : Type) [AddCommGroup V] [VectorSpace ℂ V] [InnerProductSpace_v V] (v : V) :
  ⟪v, v⟫ = ↑(⟪v, v⟫.re) := by
  apply Complex.ext
  · rfl
  · exact inner_self_im_zero v

--Level 11
--Title "Inner Product Distributes over Subtraction (Second Argument)"
--Introduction "This theorem establishes that $\langle u, v+w\rangle = \langle u,v\rangle + \langle u,w\rangle$. Although our inner product was initially defined to be linear in the first argument and conjugate symmetric, this result shows linearity in the second argument as well (technically conjugate-linearity if considering complex scalar multiplication). It’s significant because it means inner product is linear in both slots (with the appropriate conjugation in one), making computations and properties symmetric between the two arguments."
--Hint "Start from the left-hand side and use the conjugate symmetry property: $\langle u, v+w \rangle = \overline{\langle v+w, u \rangle}$. By additivity in the first argument, $\langle v+w, u \rangle = \langle v, u\rangle + \langle w, u\rangle$. Take conjugates of that sum: $\overline{\langle v, u\rangle + \langle w, u\rangle} = \overline{\langle v,u\rangle} + \overline{\langle w,u\rangle} = \langle u,v\rangle + \langle u,w\rangle$ (using conjugate symmetry again on each term). Since conjugation is injective, we conclude $\langle u, v+w\rangle = \langle u,v\rangle + \langle u,w\rangle$."
-- Inner product is additive in the second slot
theorem inner_add_right_v (u v w : V) : ⟪u, v + w⟫ = ⟪u, v⟫ + ⟪u, w⟫ := by
  apply conj_inj
  rw [conj_add]
  repeat rw [← InnerProductSpace_v.inner_conj_symm]
  exact InnerProductSpace_v.inner_add_left v w u


--Level 12
--Title "Inner Product with Zero Vector is Zero"
--Introduction "From left side, for any vector $v$, $\langle 0, v\rangle = 0$. The zero vector is orthogonal to every vector in the space. This is expected because if one argument of the inner product is the zero vector, no matter the other vector, the result (which can be thought of as “projection” of one onto the other) should be zero. This aligns with the idea that zero has no component in any direction.
--From right side, For any vector $v$, $\langle v, 0\rangle = 0$. This is the counterpart to the previous result, now in the second argument. It follows from the conjugate symmetry of the inner product: since $\langle 0,v\rangle = 0$, we must also have $\langle v,0\rangle = 0$ (because the conjugate of 0 is 0). This confirms that the zero vector is orthogonal to all vectors from either side."
--Hint "Represent the zero vector as a scaled vector: $0 = 0 \cdot 0$ (here $0$ on the right is the zero vector itself). Then use homogeneity in the first argument: $\langle 0 \cdot 0,; v \rangle = 0 * \langle 0, v\rangle$. The left side is $\langle 0,v\rangle$ and the right side is $0 * \langle 0,v\rangle = 0$. Thus $\langle 0,v\rangle = 0$."


-- Inner product with zero vector is zero
theorem inner_zero_left_v (v : V) : ⟪0, v⟫ = 0 := by
  have h : (0 : V) = (0 : ℂ) • (0 : V) := by
    rw [zero_smul_v ℂ V]
  rw [h, InnerProductSpace_v.inner_smul_left]
  rw [zero_mul]

--Hint "Similarly, for the right hand side, apply conjugate symmetry: $\langle v,0\rangle = \overline{\langle 0,v\rangle}$. We know $\langle 0,v\rangle = 0$, so its conjugate is $0$. Therefore $\langle v,0\rangle = 0$ as well."
theorem inner_zero_right_v (v : V) : ⟪v, 0⟫ = 0 := by
  rw [InnerProductSpace_v.inner_conj_symm]
  rw [inner_zero_left_v]
  exact conj_zero


--Level 13
--Title "Inner Product is Homogeneous in Second Argument"
--Introduction "This theorem shows that $\langle v, a \cdot w\rangle = \overline{a},\langle v,w\rangle$ for any scalar $a \in \mathbb{C}$. In other words, when a scalar multiplies the second argument, it can be pulled out of the inner product as its complex conjugate. This property, together with homogeneity in the first argument, means the inner product is sesquilinear (linear in one argument, conjugate-linear in the other). It’s crucial for many proofs that involve moving scalars in and out of inner products."
--Hint "We know $\langle v, a w\rangle$’s conjugate is $\overline{\langle v, a w\rangle} = \langle a w, v\rangle$ (by conjugate symmetry). Using homogeneity in the first slot, $\langle a w, v\rangle = a * \langle w, v\rangle$. Now, $a * \langle w,v\rangle$ is the same as $a * \overline{\langle v,w\rangle}$ (since $\langle w,v\rangle = \overline{\langle v,w\rangle}$). Taking the conjugate of this gives $\overline{a * \overline{\langle v,w\rangle}} = \overline{a} * \langle v,w\rangle$. Thus, $\overline{\langle v, a w\rangle} = \overline{a},\overline{\langle v,w\rangle}$, and by injectivity of conjugation, $\langle v, a w\rangle = \overline{a},\langle v,w\rangle$."
-- Homogeneity in the second slot (with conjugation)
theorem inner_smul_right_v (a : ℂ) (v w : V) :
  ⟪v, a • w⟫ = conj a * ⟪v, w⟫ := by
  apply conj_inj
  rw [conj_mull]
  rw [Complex.conj_conj]
  repeat rw [← InnerProductSpace_v.inner_conj_symm]
  exact InnerProductSpace_v.inner_smul_left a w v



variable {V : Type} [AddCommGroup V] [VectorSpace ℂ V] [DecidableEq V] [InnerProductSpace_v V]


noncomputable def norm_v (v:V) : ℝ := Real.sqrt ⟪v, v⟫.re

notation "‖" x "‖" => norm_v x

def orthogonal (u v:V) : Prop := ⟪u, v⟫= (0:ℂ)

--Level 14
--Title "Scalar Multiple of Orthogonal Vector is Orthogonal"
--Introduction "If $u$ is orthogonal to $v$ (i.e. $\langle u,v\rangle = 0$), then any scalar multiple of $u$ is also orthogonal to $v$: $\langle c\cdot u,; v\rangle = 0$ for any scalar $c$. Geometrically, scaling a vector that is perpendicular to another does not change the fact that it’s perpendicular (it only changes the length, not the angle). This property follows from the linearity of the inner product."
--Hint "Using homogeneity in the first argument, $\langle c \cdot u, v \rangle = c * \langle u, v\rangle$. If $\langle u,v\rangle = 0$, then $c * \langle u,v\rangle = c * 0 = 0$. Thus $\langle c u, v\rangle = 0$."


theorem left_smul_ortho (u v : V) (c : ℂ): orthogonal u v → orthogonal (c• u) v := by
  intro h
  dsimp [orthogonal] at *
  rw [InnerProductSpace_v.inner_smul_left]
  rw[h]
  simp

--Level 15
--Title "Orthogonality is Symmetric"
--Introduction "If $u$ is orthogonal to $v$, then $v$ is orthogonal to $u$. Formally, $\langle u,v\rangle = 0$ implies $\langle v,u\rangle = 0$. This is true because $\langle v,u\rangle$ is the complex conjugate of $\langle u,v\rangle$. If one is zero, its conjugate is also zero. This symmetry means orthogonality is a mutual relationship, just like perpendicularity in Euclidean space."
--Hint "Use conjugate symmetry of the inner product: $\langle v,u\rangle = \overline{\langle u,v\rangle}$. If $\langle u,v\rangle = 0$, then taking the conjugate yields $\overline{0} = 0$. Hence $\langle v,u\rangle = 0$ as well."



theorem ortho_swap : ∀ (u v :V), orthogonal u v → orthogonal v u := by
  intro u v h
  dsimp [orthogonal] at *
  rw [InnerProductSpace_v.inner_conj_symm,h]
  simp

-- LEVEL 1
--Level 16
--Title "Norm is Nonnegative"
--Introduction "This theorem states $0 \le |v|$ for all vectors $v$. Since norm is defined as a square root of a non-negative real number (the inner product of $v$ with itself), it is inherently nonnegative. This property is fundamental: lengths (norms) are never negative. It provides a partial validation that our definition of norm behaves like a length."
--Hint "By the inner product positivity property, $\Re(\langle v,v\rangle) \ge 0$. The real square root of a non-negative number is also non-negative. Formally, use Real.sqrt_nonneg on $\langle v,v\rangle.\mathrm{re}` to conclude $\sqrt{\langle v,v\rangle.\mathrm{re}} \ge 0$."

theorem norm_nonneg_v (v: V): 0≤ ‖v‖ := by
  unfold norm_v
  exact Real.sqrt_nonneg ⟪v,v⟫.re

-- LEVEL 2
--Level 17
--Title "Zero Norm if and only if Zero Vector"
--Introduction "This captures the definiteness property in terms of norm: $|v| = 0$ if and only if $v = 0$. It means that the only vector with zero length is the zero vector, and conversely the zero vector has zero length. This is important because it tells us the norm is positive definite (no non-zero vector can have zero length), which is one of the defining properties of a norm."
--Hint "For the forward direction ($|v| = 0 \implies v=0$): square both sides to get $|v|^2 = 0$. By definition, $|v|^2 = \langle v,v\rangle.\mathrm{re}$. So $\langle v,v\rangle.\mathrm{re} = 0$. Given $\langle v,v\rangle$ has zero imaginary part, this implies $\langle v,v\rangle = 0$. By definiteness of the inner product, $v=0`. For the reverse ($v=0 \implies |v|=0$): simply substitute $v=0$ into the definition, $|0| = \sqrt{\langle 0,0\rangle.\mathrm{re}} = \sqrt{0} = 0$."


theorem norm_zero_v (v: V): norm_v v = 0 ↔ v = 0 := by
  constructor
  intro h
  apply_fun (λ x ↦ x^2) at h
  have : (Real.sqrt ⟪v, v⟫.re)^2 = ⟪v, v⟫.re := Real.sq_sqrt (InnerProductSpace_v.inner_self_nonneg ..)
  unfold norm_v at h
  rw[this] at h
  simp at h
  have h1 : ⟪v,v⟫.im =0 := by exact inner_self_im_zero v
  have h2 : ⟪v,v⟫ =0 := by exact Eq.symm (Complex.ext (id (Eq.symm h)) (id (Eq.symm h1)))
  exact (InnerProductSpace_v.inner_self_eq_zero v).mp h2
  intro h
  --#check
  --rw[(InnerProductSpace_v.inner_self_eq_zero v).mp]
  unfold norm_v

  rw[Real.sqrt_eq_zero (InnerProductSpace_v.inner_self_nonneg (v:V))]
  rw [← InnerProductSpace_v.inner_self_eq_zero v] at h
  rw[h]
  simp

-- LEVEL 3
--Level 18
--Title "Norm of a Scaled Vector"
--Introduction "This theorem proves the homogeneity of the norm: $|a \cdot v| = |a| \cdot |v|$, where $|a|$ is the modulus (absolute value) of the scalar $a$. In real spaces this says $|a v| = |a|,|v|$ (and $|a| = a$ or $-a$ depending on sign), and in complex spaces it similarly uses the complex modulus. This property is essential for norms, showing that scaling a vector by $a$ scales its length by the factor $|a|$."
--Hint "Square both sides to avoid the square root: you want to show $|a v|^2 = |a|^2 |v|^2$. Now, $|a v|^2 = \Re(\langle a v, a v\rangle)$. Using inner product properties, $\langle a v, a v\rangle = a \overline{a} * \langle v,v\rangle$ (since one $a$ comes out from each slot, conjugated in one). Note that $a \overline{a} = |a|^2$ (the norm squared of $a$). Thus $\Re(\langle a v, a v\rangle) = |a|^2 \Re(\langle v,v\rangle)$. This shows $|a v|^2 = |a|^2 |v|^2$. Because both $|a v|$ and $|a| |v|$ are nonnegative, taking square roots yields $|a v| = |a| |v|$."

theorem sca_mul (a : ℂ) (v: V) : ‖a • v‖=‖a‖ * ‖v‖ := by
  have h1 : 0≤ ‖v‖:= norm_nonneg_v v
  have h2 : 0≤ ‖a‖ := norm_nonneg a
  have g1 : 0≤ ‖a‖ * ‖v‖ := Left.mul_nonneg h2 h1
  have g2 : 0≤ ‖a• v‖ := by exact norm_nonneg_v (a • v)
  have h3 : ‖a• v‖=‖a‖ * ‖v‖ ↔ (‖a• v‖)^2=(‖a‖ * ‖v‖)^2 :=Iff.symm (sq_eq_sq g2 g1)
  rw[h3]
  have h4 : (‖a‖ * ‖v‖)^2=‖a‖^2 * ‖v‖^2 := by ring
  have h5 := InnerProductSpace_v.inner_self_nonneg v
  have h6 := InnerProductSpace_v.inner_self_nonneg (a • v)
  rw[h4]
  unfold norm_v
  rw[Real.sq_sqrt h6]
  rw [InnerProductSpace_v.inner_smul_left]
  rw [inner_smul_right_v]
  rw[← mul_assoc]
  rw[Complex.mul_conj]
  calc (↑(Complex.normSq a) * ⟪v,v⟫).re =(Complex.normSq a) * ⟪v,v⟫.re := by exact Complex.re_ofReal_mul (Complex.normSq a) ⟪v,v⟫
    _ = ‖a‖ ^ 2 * ⟪v,v⟫.re := by rw [Complex.normSq_eq_norm_sq]

    _ = ‖a‖^2  * Real.sqrt ⟪v,v⟫.re ^ 2:= by rw[Real.sq_sqrt h5]

--Level 19
--Title "Any Vector is Orthogonal to Zero Vector"
--Introduction "This theorem confirms that for every vector $v$, $v$ is orthogonal to the zero vector: $\langle v, 0\rangle = 0$. We already separately established $\langle 0, v\rangle = 0$ and $\langle v,0\rangle = 0$, so this is consistent. It highlights that the zero vector has no component in any direction and is perpendicular to all vectors."
--Hint "This is immediate from the earlier result that $\langle v,0\rangle = 0$. No additional work is needed beyond citing that property or using the definition of orthogonal with one vector as 0."

theorem ortho_zero (v : V) : orthogonal v (0:V):= by
  unfold orthogonal
  exact inner_zero_right_v v

-- LEVEL 4
--Level 20
--Title "A Vector is Orthogonal to Itself if and only if It Is the Zero Vector"
--Introduction "This is another formulation of definiteness: $v$ is orthogonal to itself (meaning $\langle v,v\rangle = 0$) if and only if $v$ is the zero vector. In other words, no nonzero vector can be orthogonal to itself because $\langle v,v\rangle$ must be positive if $v$ is nonzero. This is crucial in inner product spaces since it guarantees positive definiteness."
--Hint "If $\langle v,v\rangle = 0$, by the inner product space’s definiteness property we directly conclude $v=0`. Conversely, if $v=0$, then trivially $\langle v,v\rangle = \langle 0,0\rangle = 0$."

theorem ortho_self_zero (v : V): orthogonal v v ↔ v = (0:V):= by
  constructor
  unfold orthogonal
  intro h
  exact (InnerProductSpace_v.inner_self_eq_zero v).mp h
  intro h
  unfold orthogonal
  exact (InnerProductSpace_v.inner_self_eq_zero v).mpr h

--LEVEL 5
--Level 21
--Title "Pythagorean Theorem for Orthogonal Vectors"
--Introduction "If $u$ and $v$ are orthogonal (i.e. $\langle u,v\rangle = 0$), then $|u+v|^2 = |u|^2 + |v|^2$. This is the Pythagorean theorem generalized to any inner product space: the square of the length of the sum equals the sum of squares of lengths, when the vectors are perpendicular. It’s a direct consequence of expanding the norm and using orthogonality to drop the cross term. This theorem is fundamental in orthogonal decomposition and is a stepping stone to more general results like the Parallelogram Law."
--Hint "Start with $|u+v|^2 = \Re(\langle u+v, u+v\rangle)$. Expand the inner product: $\langle u+v, u+v\rangle = \langle u,u\rangle + \langle u,v\rangle + \langle v,u\rangle + \langle v,v\rangle$. Using orthogonality, $\langle u,v\rangle = \langle v,u\rangle = 0$. So this simplifies to $\langle u,u\rangle + \langle v,v\rangle$. Thus $\Re(\langle u+v, u+v\rangle) = \Re(\langle u,u\rangle) + \Re(\langle v,v\rangle)$, which is $|u|^2 + |v|^2$."

theorem pythagorean (u v : V) (h : orthogonal u v) : ‖u + v‖^2 = ‖u‖^2 + ‖v‖^2 := by
  -- First, let's establish that ⟪u,v⟫ = 0 from orthogonality
  have huv : ⟪u, v⟫ = 0 := h
  -- We'll work with the squared norms
  have h1 : 0 ≤ ⟪u + v, u + v⟫.re := InnerProductSpace_v.inner_self_nonneg (u + v)
  have h2 : 0 ≤ ⟪u, u⟫.re := InnerProductSpace_v.inner_self_nonneg u
  have h3 : 0 ≤ ⟪v, v⟫.re := InnerProductSpace_v.inner_self_nonneg v

  -- Expand ‖u + v‖^2 using the definition of norm_v
  unfold norm_v
  rw [Real.sq_sqrt h1, Real.sq_sqrt h2, Real.sq_sqrt h3]

  -- Expand ⟪u + v, u + v⟫ using additivity properties
  have expand : ⟪u + v, u + v⟫ = ⟪u, u + v⟫ + ⟪v, u + v⟫ := by
    exact InnerProductSpace_v.inner_add_left u v (u + v)

  -- Further expand the right-hand side
  have expand2 : ⟪u, u + v⟫ = ⟪u, u⟫ + ⟪u, v⟫ := by
    exact inner_add_right_v u u v

  have expand3 : ⟪v, u + v⟫ = ⟪v, u⟫ + ⟪v, v⟫ := by
    exact inner_add_right_v v u v

  -- Now we have ⟪u + v, u + v⟫ = ⟪u, u⟫ + ⟪u, v⟫ + ⟪v, u⟫ + ⟪v, v⟫
  rw [expand, expand2, expand3]
  --rw [← add_assoc, ← add_assoc]
  rw [add_assoc ⟪u, u⟫ ⟪u, v⟫ (⟪v, u⟫ + ⟪v, v⟫)]
  rw [← add_assoc ⟪u, v⟫ ⟪v, u⟫ ⟪v, v⟫]
  rw [add_comm ⟪u, v⟫ ⟪v, u⟫]
  rw [add_assoc ⟪v, u⟫ ⟪u, v⟫ ⟪v, v⟫]
  --rw [← add_assoc ⟪u, u⟫ (⟪v, u⟫ + ⟪u, v⟫) ⟪v, v⟫]

  -- Use orthogonality: ⟪u, v⟫ = 0
  rw [huv]

  -- We need to show that ⟪v, u⟫ = 0 as well
  have hvu : ⟪v, u⟫ = 0 := by
    rw [InnerProductSpace_v.inner_conj_symm]
    rw [huv]
    exact conj_zero

  rw [hvu]

  -- Now we have ⟪u, u⟫ + (0 + 0) + ⟪v, v⟫
  simp


--Level 22
--Title "Norm Squared equals Inner Product (Real Part)"
--Introduction "This is a simple but useful relation: $|v|^2 = \langle v,v\rangle.\mathrm{re}$. It connects the norm back to the inner product definition. Essentially by definition $|v|^2 = \Re(\langle v,v\rangle)$, but proving it formally confirms there’s no sign ambiguity. The significance is that one can replace norm squares with inner products in proofs, which is often convenient (e.g., when applying Cauchy-Schwarz or expanding norms)."
--Hint "Unfold the definition of $‖v‖$ and square it: $|v|^2 = (\sqrt{\Re(\langle v,v\rangle)})^2$. Because $\Re(\langle v,v\rangle) \ge 0$, this just gives back $\Re(\langle v,v\rangle)$."

theorem norm_sq_eq (v : V) :  ‖v‖^2 = ⟪v,v⟫.re := by
    unfold norm_v
    rw [Real.sq_sqrt (InnerProductSpace_v.inner_self_nonneg v)]

-- LEVEL 6
--Level 23
--Title "Orthogonal Decomposition"
--Introduction "Given any two vectors $u$ and $v$ with $v \neq 0$, we can decompose $u$ into a component parallel to $v$ and a component orthogonal to $v$. Specifically, there exists a scalar $c$ and a vector $w$ such that
--u=c ⬝ v +w, with $w$ orthogonal to $v$. Moreover, $c$ is given by the formula $c = \frac{\langle u,v\rangle}{|v|^2}$ and $w = u - c \cdot v$. This theorem is essentially the projection of $u$ onto the line spanned by $v$, and $w$ is the leftover orthogonal component. It’s fundamental in linear algebra (a step in the Gram-Schmidt process) and in proving the Cauchy-Schwarz inequality."
--Hint "Define $c = \frac{\langle u,v\rangle}{|v|^2}$ and $w = u - c \cdot v$. Then proceed in steps:
--Show $c$ and $w$ are as defined: This is by construction (just say $c = \ldots$ and $w = \ldots$).
--Show $u = c \cdot v + w$: This is almost immediate from the definition of $w$ (rearrange $w = u - c v$).
--Show $w$ is orthogonal to $v$: Compute $\langle w,v\rangle = \langle u - c v,; v\rangle = \langle u,v\rangle - c \langle v,v\rangle$. Now substitute the chosen value of $c$. Since $c = \frac{\langle u,v\rangle}{\langle v,v\rangle.\mathrm{re}}$ and $\langle v,v\rangle$ is real, $\langle u,v\rangle - c \langle v,v\rangle = 0$. Don’t forget to argue $|v|^2 \neq 0$ (because $v \neq 0$) so that $c$ is well-defined."

theorem ortho_decom (u v : V) (h : v≠ 0) : ∃ c : ℂ, ∃ w : V, c=⟪u,v⟫/(‖v‖^2) ∧ w=u-(⟪u,v⟫/(‖v‖^2)) • v ∧ u = c • v + w ∧  orthogonal w v := by
  -- Define c and w as specified
  let c := ⟪u,v⟫/(‖v‖^2)
  let w := u - c • v

  -- Show that these choices work
  use c, w

  -- Split the goal into the four parts
  constructor
  · -- c = ⟪u,v⟫/(‖v‖^2)
    rfl

  constructor
  · -- w = u - (⟪u,v⟫/(‖v‖^2)) • v
    rfl

  constructor
  · -- u = c • v + w
    -- This follows from the definition of w
    simp [w]

  · -- ⟪w,v⟫ = 0
    -- Expand w
    simp [w, c]

    unfold orthogonal
    -- Use linearity of inner product
    rw [inner_minus_left]
    rw [InnerProductSpace_v.inner_smul_left]

    -- We need to show ⟪u,v⟫ - (⟪u,v⟫/‖v‖^2) * ⟪v,v⟫ = 0
    -- First, let's establish that ‖v‖^2 ≠ 0
    have hv_norm_pos : ‖v‖^2 ≠ 0 := by
      by_contra g
      rw [sq_eq_zero_iff] at g
      suffices ‖v‖≠ 0 by contradiction
      intro hcontra
      have : v = 0 := (norm_zero_v v).mp hcontra
      exact h this

    -- Also, ‖v‖^2 = ⟪v,v⟫.re (from the definition of norm)

    -- Since ⟪v,v⟫ is real, we have ⟪v,v⟫ = ⟪v,v⟫.re
    have hvv_real : ⟪v,v⟫ = ↑(⟪v,v⟫.re) := inner_self_re_v V v


    unfold norm_v

    have teq :=Real.sq_sqrt (InnerProductSpace_v.inner_self_nonneg v)
    have teqq :(Real.sqrt ⟪v,v⟫.re ^ 2 :ℂ) = ⟪v,v⟫.re := by
      norm_cast
    rw[teqq]

    rw [← hvv_real]

    -- We have ⟪u,v⟫ - (⟪u,v⟫/⟪v,v⟫) * ⟪v,v⟫
    -- Convert to common denominator
    have : ⟪u,v⟫ - ⟪u,v⟫ / ↑⟪v,v⟫.re * ⟪v,v⟫ = 0 := by
      -- First convert ⟪v,v⟫.re to complex
      have hre_ne_zero : (⟪v,v⟫.re : ℂ) ≠ 0 := by
        --rw [← hvv_real] at hv_norm_pos
        unfold norm_v at hv_norm_pos
        rw [← norm_sq_eq] at hv_norm_pos
        simp
        intro hcontra
        unfold norm_v at hv_norm_pos
        rw [hcontra] at hv_norm_pos
        simp at hv_norm_pos
      -- Now do the algebra
      field_simp [hre_ne_zero]
      rw [inner_self_real]
      simp

    rw [← hvv_real] at this
    exact this
-- Direct proof using definition


theorem le_of_sq_le_sq {a : ℝ} {b : ℝ} (h : a^2 ≤ b ^2 ) (hb : 0≤ b) :a ≤ b :=
  le_abs_self a |>.trans <| abs_le_of_sq_le_sq h hb

-- LEVEL 7
--Level 24
--Title "Cauchy-Schwarz Inequality"
--Introduction "The Cauchy–Schwarz inequality asserts that for all vectors $u,v$ in an inner product space,
--∣⟨u,v⟩∣≤∥u∥⋅∥v∥. This is one of the most important inequalities in mathematics. It geometrically means the absolute value of the inner product (related to the cosine of the angle between $u$ and $v$) is bounded by the product of lengths, ensuring the cosine is between -1 and 1. It is also the key to proving the triangle inequality for norms and to many other results (like showing that the inner product induces a valid norm)."
--Hint "Consider two cases:
--Case 1: If $v = 0$, then $\langle u,v\rangle = \langle u,0\rangle = 0$, so $|\langle u,v\rangle| = 0$ and the inequality holds since $|u||v| = |u| * 0 = 0$.
--Case 2: If $v \neq 0$, use the orthogonal decomposition of $u$ relative to $v$. Write $u = c v + w$ with $w \perp v$ (where $c = \frac{\langle u,v\rangle}{|v|^2}$). Now $\langle u,v\rangle = \langle c v + w, v\rangle = c \langle v,v\rangle$ (since $w$ is orthogonal to $v$). Thus $|\langle u,v\rangle| = |c| \cdot |v|^2$. On the other hand, $|u|^2 = |c v + w|^2 = |c|^2 |v|^2 + |w|^2 \ge |c|^2 |v|^2$ (by the Pythagorean theorem). So $|u|^2 \ge |c|^2 |v|^2$. Taking square roots gives $|u| \ge |c| |v|$. Multiply both sides by $|v|$ to get $|u||v| \ge |c| |v|^2 = |\langle u,v\rangle|$."

theorem Cauchy_Schwarz (u v : V) : ‖⟪u,v⟫‖ ≤ ‖u‖ * ‖v‖ := by
  by_cases v_zero : v = 0

  case pos =>
    rw[v_zero]
    rw [inner_zero_right_v]
    have h := norm_zero_v (0:V)
    simp at h
    rw[h]
    simp
  case neg =>

    obtain ⟨c, w, h1, _h2, h3, h4⟩ := ortho_decom u v v_zero
    have h5:= left_smul_ortho v w c (ortho_swap w v h4)
    have g1 := norm_nonneg_v u
    have g2 := norm_nonneg_v v

    have g3 : 0≤ ‖u‖ * ‖v‖ := by exact Left.mul_nonneg g1 g2
    have g5 : 0≤ ‖w‖ := norm_nonneg_v w
    suffices  ‖⟪u,v⟫‖^2 ≤ ‖u‖^2 * ‖v‖^2  by
      have ts : ‖u‖ ^ 2 * ‖v‖^2 = (‖u‖ * ‖v‖)^2 := by ring
      rw[ts] at this

      exact le_of_sq_le_sq this g3

    have h6 := pythagorean (c• v) w h5
    nth_rw 2 [h3]
    rw [h6]
    have v_norm_zero : ‖v‖≠ 0 := by
      by_contra h
      rw[norm_zero_v v] at h
      contradiction

    have kt :  ‖c • v‖ ^ 2= ‖⟪u,v⟫‖^2/‖v‖^2 := by
      -- Use the scalar multiplication property for norms
      rw [sca_mul c v]
      -- Now we have (‖c‖ * ‖v‖)^2 = ‖c‖^2 * ‖v‖^2
      ring_nf
      -- Substitute the definition of c
      rw [h1]  -- c = ⟪u,v⟫/(‖v‖^2)
      -- We need to show ‖⟪u,v⟫/(‖v‖^2)‖^2 * ‖v‖^2 = ‖⟪u,v⟫‖^2/‖v‖^2
      rw [@norm_div]
      --rw [Complex.norm_div]
      -- This gives us (‖⟪u,v⟫‖ / ‖‖v‖^2‖)^2 * ‖v‖^2
      simp only [@norm_pow, Complex.norm_real, abs_sq]
      -- Since ‖v‖^2 is real and non-negative, ‖‖v‖^2‖ = ‖v‖^2
      simp
      -- Now we have (‖⟪u,v⟫‖^2 / (‖v‖^2)^2) * ‖v‖^2 = ‖⟪u,v⟫‖^2 / ‖v‖^2
      field_simp [v_norm_zero]
      ring
    rw[kt]
    rw[add_mul]
    field_simp [v_norm_zero]


-- LEVEL 8
--Level 25
--Title "Triangle Inequality"
--Introduction "This theorem proves the triangle inequality for the norm: $|u+v| \le |u| + |v|$. Geometrically, this means the straight-line distance (norm of $u+v$) between the endpoints of vectors $u$ and $v$ is not greater than the two-step path that goes along $u$ and then along $v$. It’s a cornerstone property of norms, showing that the function $| \cdot |$ indeed gives a metric on the vector space."
--Hint "Square both sides of the desired inequality to avoid dealing with the absolute value of inner products directly: prove $|u+v|^2 \le (|u| + |v|)^2$. Expanding the right side yields $|u|^2 + 2|u||v| + |v|^2$. Expanding the left side: $|u+v|^2 = \Re(\langle u+v, u+v\rangle) = |u|^2 + 2\Re(\langle u,v\rangle) + |v|^2$. Now, use the fact that $\Re(\langle u,v\rangle) \le |\langle u,v\rangle| \le |u||v|$ (by Cauchy–Schwarz and because the real part of any complex number is bounded by its modulus). Thus $2\Re(\langle u,v\rangle) \le 2|u||v|$. This gives $|u+v|^2 \le |u|^2 + 2|u||v| + |v|^2$, which is $(|u|+|v|)^2$. Taking nonnegative square roots yields the triangle inequality."

theorem triangle_v (u v :V) : ‖u+v‖≤ ‖u‖ + ‖v‖ := by
  suffices  ‖u+v‖^2 ≤ (‖u‖ + ‖v‖)^2  by
    exact le_of_sq_le_sq this (Left.add_nonneg (norm_nonneg_v u) (norm_nonneg_v v))
  have h1 : 0 ≤ ⟪u + v, u + v⟫.re := InnerProductSpace_v.inner_self_nonneg (u + v)

  -- Expand ‖u + v‖^2 using the definition of norm_v


  have : ‖u+v‖ =Real.sqrt ⟪u + v,u + v⟫.re := by rfl
  rw[this]
  rw [Real.sq_sqrt h1]

  -- Expand ⟪u + v, u + v⟫ using additivity properties
  have expand : ⟪u + v, u + v⟫ = ⟪u, u + v⟫ + ⟪v, u + v⟫ := by
    exact InnerProductSpace_v.inner_add_left u v (u + v)

  -- Further expand the right-hand side
  have expand2 : ⟪u, u + v⟫ = ⟪u, u⟫ + ⟪u, v⟫ := by
    exact inner_add_right_v u u v

  have expand3 : ⟪v, u + v⟫ = ⟪v, u⟫ + ⟪v, v⟫ := by
    exact inner_add_right_v v u v

  -- Now we have ⟪u + v, u + v⟫ = ⟪u, u⟫ + ⟪u, v⟫ + ⟪v, u⟫ + ⟪v, v⟫
  rw [expand, expand2, expand3]
  rw [add_assoc]
  nth_rw 2 [← add_assoc]


  have h5 : (⟪u,v⟫ + ⟪v,u⟫).re =2 *⟪u,v⟫.re := by
    nth_rw 2 [InnerProductSpace_v.inner_conj_symm]
    simp
    ring


  have h6 : ⟪u,v⟫.re ≤ ‖⟪u,v⟫‖ := by
    set c:= ⟪u,v⟫
    unfold norm
    have q1 : 0 ≤ c.im^2 := sq_nonneg c.im
    have q2 : c.im^2 = c.im * c.im := by rw [@sq]
    rw[q2] at q1
    have p1 : c.re * c.re ≤ c.re*c.re + c.im *c.im := le_add_of_nonneg_right q1
    have q3 :0 <= c.re^2 := sq_nonneg c.re
    have q4 : c.re^2=c.re*c.re := by rw[@sq]
    rw[q4] at q3
    have q5 : 0≤ c.re*c.re + c.im*c.im := add_nonneg q3 q1
    have q6 : (Real.sqrt (c.re * c.re + c.im * c.im))^2 = c.re * c.re + c.im * c.im := by
      exact Real.sq_sqrt q5
    rw[← q6] at p1
    nth_rw 1 [← q4] at p1
    have q7 := Real.sqrt_nonneg (c.re * c.re + c.im * c.im)
    exact le_of_sq_le_sq p1 q7


  have h7 := Cauchy_Schwarz u v
  have h8: (⟪u,u⟫ + (⟪u,v⟫ + ⟪v,u⟫ + ⟪v,v⟫)).re = ⟪u,u⟫.re + ((⟪u,v⟫ + ⟪v,u⟫).re + ⟪v,v⟫.re) := rfl
  rw [h8]
  have h9: ⟪u,u⟫.re + ((⟪u,v⟫ + ⟪v,u⟫).re + ⟪v,v⟫.re) = ⟪u,u⟫.re  + ⟪v,v⟫.re + (⟪u,v⟫ + ⟪v,u⟫).re := by ring
  rw [h9]
  rw[h5]
  have h10 : ⟪u,u⟫.re  + ⟪v,v⟫.re + 2*⟪u,v⟫.re ≤ ⟪u,u⟫.re  + ⟪v,v⟫.re + 2*‖⟪u,v⟫‖ := by linarith [h6]
  apply le_trans h10
  have h11 : ⟪u,u⟫.re  + ⟪v,v⟫.re + 2*‖⟪u,v⟫‖ ≤ ⟪u,u⟫.re  + ⟪v,v⟫.re + 2*(‖u‖*‖v‖) := by linarith [h7]
  apply le_trans h11
  rw [add_sq]
  repeat rw[norm_sq_eq]
  ring_nf
  exact Preorder.le_refl (⟪u,u⟫.re + ⟪v,v⟫.re + ‖u‖ * ‖v‖ * 2)
